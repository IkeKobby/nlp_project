# -*- coding: utf-8 -*-
"""nlp_preoject_helper_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HHTq_AE2NfVvwYNeWJYXDZ2JMPVfU4Ta
"""

### to reduce the long installation outputs
from IPython.display import clear_output


## importing dependencies
import json
from pathlib import Path
import zipfile

import tensorflow as tf
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

## word embedding model
import gensim

## filtering out warnings
import warnings
warnings.filterwarnings('ignore')

def original_data():
  """
  This contains the link to the orginal dataset, downloads 
  it into a directory here, 'fquad` on colab.
  """
  !mkdir fquad
  #!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json
  # !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json

  !wget https://github.com/ndah-e/Natural-language-processing-course/raw/main/notebooks/data/fquad1.0.zip -O fquad/fquad1.0.zip
  unzip =zipfile.ZipFile('fquad/fquad1.0.zip')
  unzip.extractall('fquad/')
  unzip.close()

  clear_output()
original_data()

def read_fquad(path):
  """
  Give the path of the current directory of the json file.
  The function returns `context`, `questions`, and `answers` in a list format from the json document.
  """
  path = Path(path)
  with open(path, 'rb') as f:
      squad_dict = json.load(f)

  contexts = []
  questions = []
  answers = []
  for group in squad_dict['data']:
      for passage in group['paragraphs']:
          context = passage['context']
          for qa in passage['qas']:
              question = qa['question']
              for answer in qa['answers']:
                  contexts.append(context)
                  questions.append(question)
                  answers.append(answer)

  return contexts, questions, answers

train_contexts, train_questions, train_answers = read_fquad('fquad/train.json')
val_contexts, val_questions, val_answers = read_fquad('fquad/valid.json')

def add_end_idx(answers, contexts):
  """
  The functions fixes the missing end index in the answers.
  """
  for answer, context in zip(answers, contexts):

    gold_text = answer['text']
    start_idx = answer['answer_start']
    end_idx = start_idx + len(gold_text.split())
    answer['answer_end'] = int(end_idx)

      # sometimes the answers are off by a character or two – we fix this 
    # if context[start_idx:end_idx] == gold_text:
    #     answer['answer_end'] = end_idx
    # elif context[start_idx-1:end_idx-1] == gold_text:
    #     answer['answer_start'] = start_idx - 1
    #     answer['answer_end'] = end_idx - 1     # When the gold label is off by one character
        
    # elif context[start_idx-2:end_idx-2] == gold_text:
    #     answer['answer_start'] = start_idx - 2
    #     answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters

    # elif context[start_idx-3:end_idx-3] == gold_text:
    #   answer['answer_start'] = start_idx - 3
    #   answer['answer_end'] = end_idx - 3

    # elif context[start_idx-4:end_idx-4] == gold_text:
    #   answer['answer_start'] = start_idx - 4
    #   answer['answer_end'] = end_idx - 4

    # elif context[start_idx-5:end_idx-5] == gold_text:
    #   answer['answer_start'] = start_idx - 5
    #   answer['answer_end'] = add_end_idx - 5

# add_end_idx(train_answers, train_contexts)
# add_end_idx(val_answers, val_contexts)

def build_dataframe(train_contexts, 
                    train_questions, 
                    train_answers
                    ):
  """
  The function builds a dataframe for the output of the read_fquad method.
  """
  samples = []
  for i in range(len(train_contexts)):
    context = train_contexts[i]
    question = train_questions[i]
    answer = train_answers[i]['text']
    ans_start = train_answers[i]['answer_start']
    ans_end = train_answers[i]['answer_end']

    samples.append([context, question, answer, ans_start, ans_end])

  df = pd.DataFrame(samples, columns = ['context', 'questions', 'answers', 'answer_start', 'answer_end'])
  return df

# train_data = build_dataframe(train_contexts, train_questions, train_answers)
# val_data = build_dataframe(val_contexts, val_questions, val_answers)

def lower(df):
  """
  This fucntion lower all upper cases in the text columns
  """
  df['context'] = [i.lower() for i in df.context]
  df['questions'] = [i.lower() for i in df.questions]
  df['answers'] = [i.lower() for i in df.answers]

  return df

import re

def fix_answer_index(df):
  """
  Due to spotted positioning of answer tokens, this method generates 
  the right positioning of the end and start tokens for the answers
  Pass in the dataframe with the `context`, `answers` tokens to fix it. 
  """
  for i in range(len(df)):

    context_tokens = re.split(",| |_|-|!|\.|\(|\)|’|=|'", df.context[i])
    answer_tokens = re.split(",| |_|-|!|\.|\(|\)|’|=|'", df.answers[i])
    #print(i)
    for ans_token in context_tokens:
      ans_start = int(context_tokens.index(answer_tokens[0])) 
      ans_end = int(ans_start + len(answer_tokens))

      df['answer_start'].loc[i] = ans_start
      df['answer_end'].loc[i] = ans_end
  return df

# train_data_fixed = fix_answer_index(train_data)
# va_data_fixed = fix_answer_index(val_data)

#train_data_fixed.to_csv('train_text.csv', index = False)
#va_data_fixed.to_csv('val_text.csv', index = False)

# train_idx = int(len(train_contexts)*0.55) ### to reduce training complexity, we use a fraction of the dataset
# val_idx = int(len(val_contexts)*0.65)

# train_contexts = train_contexts[:train_idx]
# train_questions = train_questions[:train_idx]
# train_answers = train_answers[:train_idx]


# val_contexts = val_contexts[:val_idx]
# val_questions = val_questions[:val_idx]
# val_answers = val_answers[:val_idx]

# def train_val_split(data):
#   idx = int(len(data) * 0.7)
#   train_questions = data.questions[:idx]
#   #train_answers = data.answers[:idx]
#   train_contexts = data.context[:idx]

#   val_questions = data.questions[idx:].reset_index(drop=True)
#   #val_answers = data.answers[idx:].reset_index(drop=True)
#   val_contexts = data.context[idx:].reset_index(drop=True)

#   train_data = data[:idx]
#   val_data = data[idx:].reset_index(drop=True)

#   return train_contexts, train_questions, val_contexts, val_questions, train_data, val_data
  

# train_contexts, train_questions, val_contexts, val_questions, train_data, val_data = train_val_split(data_frame)

# def add_token_positions(encodings, answers):
#     start_positions = []
#     end_positions = []
#     for i in range(len(answers)):
#         start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))
#         end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))

#         # if start position is None, the answer passage has been truncated
#         if start_positions[-1] is None:
#             start_positions[-1] = tokenizer.model_max_length
#         if end_positions[-1] is None:
#             end_positions[-1] = tokenizer.model_max_length

#     encodings.update({'start_positions': start_positions, 'end_positions': end_positions})

# add_token_positions(train_encodings, train_answers)
# add_token_positions(val_encodings, val_answers)

# train_dataset = tf.data.Dataset.from_tensor_slices((
#     {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},
#     {key: train_encodings[key] for key in ['start_positions', 'end_positions']}
# ))
# val_dataset = tf.data.Dataset.from_tensor_slices((
#     {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},
#     {key: val_encodings[key] for key in ['start_positions', 'end_positions']}
# ))

